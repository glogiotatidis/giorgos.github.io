<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>giorgos</title><link href="https://glogiotatidis.github.io/" rel="alternate"></link><link href="https://glogiotatidis.github.io/feeds/misc.atom.xml" rel="self"></link><id>https://glogiotatidis.github.io/</id><updated>2015-10-13T00:00:00+03:00</updated><entry><title>Fleet job to remove unused docker images</title><link href="https://glogiotatidis.github.io/fleet-job-to-remove-unused-docker-images.html" rel="alternate"></link><updated>2015-10-13T00:00:00+03:00</updated><author><name>Giorgos</name></author><id>tag:glogiotatidis.github.io,2015-10-13:fleet-job-to-remove-unused-docker-images.html</id><summary type="html">&lt;p&gt;Engagement Engineering, the team that I'm part of at
&lt;a href="https://www.mozilla.org"&gt;Mozilla&lt;/a&gt;, runs two &lt;a href="http://deis.io"&gt;Deis&lt;/a&gt;
clusters on AWS to host important websites including www.mozilla.org.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://deis.io"&gt;Deis&lt;/a&gt; is a Heroku-inspired PaaS which utilizes
&lt;a href="http://coreos.com/"&gt;CoreOS&lt;/a&gt; and &lt;a href="http://docker.io/"&gt;Docker&lt;/a&gt;. It's a
great open-source project,
&lt;a href="https://github.com/deis/deis"&gt;developed in the public&lt;/a&gt;, with a great
Community and commercially backed by Engine Yard.&lt;/p&gt;
&lt;p&gt;Apps on Deis run within Docker containers which run on CoreOS machines
that form the Deis cluster. Each new release of your code, i.e. each
new &lt;em&gt;deis pull&lt;/em&gt; or &lt;em&gt;git push&lt;/em&gt;, creates a new Docker image that is
stored in the internal Deis Docker Registry and downloaded onto the
CoreOS machines that are scheduled to run your code.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/mozilla/bedrock"&gt;Bedrock&lt;/a&gt; (the project name for
www.mozilla.org) is a big website. It's Docker image weights about
600MiB with all the translations, images and python dependencies. On
every new release of Bedrock, which happens almost every day, the
machines of our cluster pulled and stored about half a gig of space
only for bedrock.&lt;/p&gt;
&lt;p&gt;Since all the Docker images are stored in the internal Deis Docker
Registry anyway there is no need to store the stopped older docker
containers or their images in the individual CoreOS instances. If we
need to revert to an older release of Bedrock the machines will pull
the needed layers from the Deis Docker Registry again.&lt;/p&gt;
&lt;p&gt;We built and deployed a &lt;a href="https://github.com/coreos/fleet"&gt;Fleet&lt;/a&gt;
Service to remove the unused images:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;docker-cleanup.service&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[Unit]&lt;/span&gt;
&lt;span class="na"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Clean up unused docker containers and images&lt;/span&gt;

&lt;span class="k"&gt;[Service]&lt;/span&gt;
&lt;span class="na"&gt;Type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;oneshot&lt;/span&gt;
&lt;span class="na"&gt;ExecStart&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/bin/bash -c &amp;#39;CONTAINERS=`docker ps -a -q -f status=exited`; if [[ ! -z $CONTAINERS ]]; then docker rm -v $CONTAINERS; fi; IMAGES=`docker images -f dangling=true -q`; if [[ ! -z $IMAGES ]]; then docker rmi $IMAGES; fi; echo &amp;quot;Completed docker image cleanup.&amp;quot;&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;[Install]&lt;/span&gt;
&lt;span class="na"&gt;WantedBy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;multi-user.target&lt;/span&gt;

&lt;span class="k"&gt;[X-Fleet]&lt;/span&gt;
&lt;span class="na"&gt;Global&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The Service checks if there're any containers which are &lt;em&gt;exited&lt;/em&gt; and
removes them. Similarly checks if there are any &lt;em&gt;dangling&lt;/em&gt; docker
images and removes them too. Prints the &lt;em&gt;Completed docker image
cleanup&lt;/em&gt; text in the end so we can check through the
&lt;a href="https://deadmanssnitch.com/"&gt;Dead Mans Snitch&lt;/a&gt; that the job run.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;X-fleet&lt;/em&gt; directive instructs Fleet to run this service on each
and every machine of the cluster.&lt;/p&gt;
&lt;p&gt;You can run this service and cleanup the images immediatelly with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fleetctl start docker-cleanup.service
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also built a timer unit to schedule daily cleanups to keep things
clean and tidy without our interaction:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;docker-cleanup.timer&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;[Unit]&lt;/span&gt;
&lt;span class="na"&gt;Description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;Trigger clean up of unused docker containers and images&lt;/span&gt;

&lt;span class="k"&gt;[Timer]&lt;/span&gt;
&lt;span class="na"&gt;OnCalendar&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;daily&lt;/span&gt;

&lt;span class="k"&gt;[X-Fleet]&lt;/span&gt;
&lt;span class="na"&gt;Global&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After loading the cleanup service, load and start the timer&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fleetctl load docker-cleanup.service
fleetctl load docker-cleanup.timer
fleetctl start docker-cleanup.timer
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and the docker-cleanup.service will run &lt;em&gt;daily&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note that &lt;em&gt;fleetctl list-units&lt;/em&gt; will list your
&lt;em&gt;docker-cleanup.service&lt;/em&gt; as &lt;em&gt;dead&lt;/em&gt; which is OK. It will run as
expected when the &lt;em&gt;waiting&lt;/em&gt; &lt;em&gt;docker-cleanup.timer&lt;/em&gt; unit triggers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;core@ip-10-0-0-0 ~ $ fleetctl list-units
UNIT                    MACHINE         ACTIVE      SUB
docker-cleanup.service          1c919e41.../10.0.0.0    inactive    dead
docker-cleanup.timer            1c919e41.../10.0.0.0    active      waiting
&lt;/pre&gt;&lt;/div&gt;</summary><category term="Mozilla"></category><category term="Docker"></category><category term="ops"></category></entry><entry><title>List the extensions of files under a directory</title><link href="https://glogiotatidis.github.io/list-the-extensions-of-files-under-a-directory.html" rel="alternate"></link><updated>2015-06-15T00:00:00+03:00</updated><author><name>Giorgos</name></author><id>tag:glogiotatidis.github.io,2015-06-15:list-the-extensions-of-files-under-a-directory.html</id><summary type="html">&lt;p&gt;I want to list the extensions of filenames in a directory, sorted and unique.&lt;/p&gt;
&lt;p&gt;First list all the files, leaving out directories using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;find -type f .
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then use &lt;a href="https://en.wikipedia.org/wiki/AWK"&gt;awk&lt;/a&gt; to get the last three characters of each line, i.e. filename.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;find . -type f | awk &amp;#39;{ print substr( $0, length($0) - 3, 4) }&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;AWK's &lt;code&gt;substr&lt;/code&gt; function extracts four characters (last argument) of string $0 (first argument) starting at full lenth of the string minus three characters (second argument).&lt;/p&gt;
&lt;p&gt;This will list the extensions from all files in the current directory and all directories below.&lt;/p&gt;
&lt;p&gt;Now &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq&lt;/code&gt; the output of the previous command for a short list all of different extensions used.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;find . -type f | awk &amp;#39;{ print substr( $0, length($0) - 3, 4) }&amp;#39; | sort | uniq
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bear in mind that &lt;code&gt;sort&lt;/code&gt; has to come before &lt;code&gt;uniq&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Lastly I'll translate to lower case all the extensions, before sort and uniq, to make the filtering case insensitive:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;find . -type f | awk &amp;#39;{ print substr( $0, length($0) - 3, 4) }&amp;#39; | tr &amp;#39;[:upper:]&amp;#39; &amp;#39;[:lower:]&amp;#39; | sort | uniq
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Final output of the command is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="na"&gt;.avi&lt;/span&gt;
&lt;span class="na"&gt;.idx&lt;/span&gt;
&lt;span class="na"&gt;.jpg&lt;/span&gt;
&lt;span class="na"&gt;.m4v&lt;/span&gt;
&lt;span class="na"&gt;.mkv&lt;/span&gt;
&lt;span class="na"&gt;.mp4&lt;/span&gt;
&lt;span class="na"&gt;.nfo&lt;/span&gt;
&lt;span class="na"&gt;.png&lt;/span&gt;
&lt;span class="na"&gt;.smi&lt;/span&gt;
&lt;span class="na"&gt;.srt&lt;/span&gt;
&lt;span class="na"&gt;.sub&lt;/span&gt;
&lt;span class="na"&gt;.txt&lt;/span&gt;
&lt;span class="na"&gt;.url&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="ops"></category></entry><entry><title>Don't hesitate to get a VPS to do your work.</title><link href="https://glogiotatidis.github.io/dont-hesitate-to-get-a-vps-to-do-your-work.html" rel="alternate"></link><updated>2015-05-05T00:00:00+03:00</updated><author><name>Giorgos</name></author><id>tag:glogiotatidis.github.io,2015-05-05:dont-hesitate-to-get-a-vps-to-do-your-work.html</id><summary type="html">&lt;p&gt;This is not going to be one of these posts about how I got a super slim, slick machine or tablet and I moved all my work in the cloud. Fortunately this is old news and nobody talks about it. That being said I have been working on VPS machines for the last 6 months for some tasks.&lt;/p&gt;
&lt;p&gt;When your bandwidth is not enough for what you've to do renting a VPS is a great option. I've seen speeds up to 200MB/s on the cheap &lt;a href="http://digitalocean.com/"&gt;DigitalOcean&lt;/a&gt; or &lt;a href="http://vultr.com/"&gt;Vultr&lt;/a&gt; VPS, with the later being faster most of the time. Want to try &lt;a href="http://docker.io/"&gt;Docker&lt;/a&gt;? Check out a couple images? You want to upload the Docker image you just built to the docker hub? It's just a matter of seconds instead of minutes or even hours some times. Spawing a VPS with 20GB and fast network for a full work day will cost you about 16 dollar cents. Once you're done you can just shut it off. If you need to do this often and you want to have some packages installed and the environment configured to your taste you can create a snapshot of your machine and fire it back up when needed. It's a nice opportunity to learn &lt;a href="http://ansible.com/"&gt;Ansible&lt;/a&gt; scripting too.&lt;/p&gt;
&lt;p&gt;Right now I'm working on my first Firefox for Android (aka Fennec) patch. It's two lines of super simple Javascript. Yet I still have to download all Fennec sources, create a proper build environment and build Fennec. I'm not a full time Fennec Developer and I don't want to have this stack on my machine. It takes space to store, time to download the needed packages and CPU cycles of my under-powered mobile CPU. Vultr has a 'Storage Series' line that won't break the bank. For $0.015 per hour I got a 250GB storage, 1GB Ram machine. I'm going to use it for a couple of days to build and test my patch and then just snapshot it for later use. Snapshot storing on Vultr is free at the moment.&lt;/p&gt;
&lt;p&gt;All in all getting a throwaway VPS to do some tasks that take loads of time on your machine and with your internet connection makes sense. You will need one to five minutes to get it running from the moment you hit deploy and you can instantly destroy it anytime. It's cheap, fast and easy.&lt;/p&gt;</summary></entry><entry><title>Duplicity scp backend weirdness</title><link href="https://glogiotatidis.github.io/duplicity-scp-backend-weirdness.html" rel="alternate"></link><updated>2015-04-28T00:00:00+03:00</updated><author><name>Giorgos</name></author><id>tag:glogiotatidis.github.io,2015-04-28:duplicity-scp-backend-weirdness.html</id><summary type="html">&lt;p&gt;I'm a big fan of &lt;a href="http://duplicity.nongnu.org/"&gt;Duplicity&lt;/a&gt; and I use to backup my laptop to my office based server over &lt;code&gt;scp&lt;/code&gt; which in its turn encrypts everything with &lt;a href="https://en.wikipedia.org/wiki/EncFS"&gt;EncFS&lt;/a&gt; and syncs everything up to my &lt;a href="http://spideroak.com/"&gt;SpiderOak&lt;/a&gt; account. I was going under a typical maintenance today and I run &lt;code&gt;duplicity collection-status&lt;/code&gt; to see how many backup sets I have.&lt;/p&gt;
&lt;p&gt;Surprisingly duplicity returned only one full backup set more than a year old and no other backup sets. After some digging I found that the returned backup set was indeed the only full set I had but besides that I had many incremental sets. Re-running the collection-status command returns some of the incremental backup sets and running it for a third time I get different -smaller- number of sets. My initial thought was that duplicity broke due to the large number of incremental sets or maybe EncFS cannot handle that many files in the same directory.&lt;/p&gt;
&lt;p&gt;Fortunately duplicity has a very verbose log setting &lt;code&gt;-v 9&lt;/code&gt; which returned interesting SSH logs&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ssh: [chan 1] Sesch channel 1 request ok
ssh: [chan 1] EOF received (1)
ssh: [chan 1] EOF sent (1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I tried the &lt;code&gt;--timeout&lt;/code&gt; flag to increase timeout with no luck. Finally running duplicity with the &lt;code&gt;sftp&lt;/code&gt; backend made the problem go away. All my backup sets were available and nothing strange in the SSH logs. I do not have any hard facts on why I chose to go with &lt;code&gt;scp&lt;/code&gt; backend in the first place but it appears to be less stable compared to &lt;code&gt;sftp&lt;/code&gt;, albeit this is the first time I run into problems with it.&lt;/p&gt;</summary><category term="ops"></category></entry></feed>